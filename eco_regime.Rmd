---
title: "Economic Regime Classification"
output:
  html_document:
    df_print: paged
---

```{r include=FALSE}
library("xts")
library("readxl")
#####classification#####
library(PerformanceAnalytics)
library(dygraphs)
library(GGally)
#####k-means#####
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(dendextend) # for comparing two dendrograms
load("eco_data.RData")
Economic_Regime_Data <- read_excel("Economic Regime Data.xlsx", sheet = "New Data", skip = 1, n_max = 2)
des = as.vector(t(Economic_Regime_Data[1,-1]))
# index = as.vector(t(Economic_Regime_Data[2,-1]))
# ticker names, input value
indices = c("EHGDUS Index",	"CPI YOY Index",	"CPI CHNG Index",	"EHUPUS Index",	"IP CHNG Index",	"NHSPATOT Index",	"NFP TCH Index"	,"TMNOCHNG Index",	"LEI TOTL Index",	"PITL YOY Index",	"CICRTOT Index",	"USCABAL Index","M2% YOY Index")
indices_des = cbind(indices, des)
```

## 1. Perparing Data

Totally we have totally 13 indices reflecting US economic market performance in this study. Those indices are listed in the code chunk below, in which you will see the name of each index and what it represent for. 

We should notice that the frequency of some series vary from each other. To solve the problem, the frequency of all indices used in this study was chosen as quarterly. Besides, to solve the inconsistency of time length, i.e. different starting time of all indices, I used the subset of series which starts in 1960 Q1 and ends in 2019 Q4, since this series has the largest common length. 

All index series are standardized before we do any statistical analysis, that is, centering each series (makes its mean equal to 0) and multiplying a constant to each series to make its variances equal to 1.
For example, the value scale of Private Housing Units Permits Total SAAR (thousands) is much greater than else. By standardized the data series, we may ignore how magnificent of its value but the relationship between it and other series. Taking advantages of those internal relationship, we can group economic time periods into different regimes and learn how to make strategies for different economic regimes. 

```{r}
#remove NA
dat.all = na.omit(dat) -> df
dygraph(dat.all, main = "Original Data")%>% dyRangeSelector()
```

```{r}
#standardized
dygraph(scale(dat.all), main = "Standardized Data")%>% dyRangeSelector()
```
 
```{r}
indices_des
```


## Data Correlation

Through looking into the correlation between each pairs of standardized indices, we can detect that some of indices are quiet closely related to each other. Since we already have 13 indices 
Therefore, we can use some dimension reduction techniques in statistics like PCA.

```{r}
df = scale(df)
cor.matrix = cor(as.data.frame(df))
cor.pairs = which(cor.matrix>0.4&cor.matrix!=1, arr.ind=TRUE)
cor.pairs = unique(t(apply(cor.pairs, 1, sort)))
colnames(cor.pairs) <- c("row", "col")
for (i in 1:nrow(cor.pairs)) {
  p = c(cor.pairs[i, "row"], cor.pairs[i, "col"])
  x = plot(df[,p], main = paste(colnames(cor.matrix)[p], collapse=" vs. "),
           legend.loc = "topleft",)
  #addLegend("topleft", colnames(cor.matrix)[p] , col=1:2, lty=1,lwd=2)
  print(x)
  #cat("The correlation between", indices_des[p[1],"des"], "and", indices_des[p[2],"des"], "is", 
  #    round(cor.matrix[p[1],p[2]],3),".\n")
  cat("The correlation between", paste(colnames(cor.matrix)[p], collapse=" and "), "is", 
      round(cor.matrix[p[1],p[2]],3),".\n")
}
```

## Classification 

### Principle Component Analaysis (PCA)

Monthly data is too repetitious for PCA, as we aim to looking into different historic economic regimes rather than a specific month. 
```{r include=FALSE}
#PCA
ep <- endpoints(df,'years')
df.yearly = period.apply(df,INDEX=ep, FUN=mean)
df.yearly = as.data.frame(scale(df.yearly))
row.names(df.yearly) = format(as.Date(row.names(df.yearly)),"%Y")
PCdf = prcomp(df.yearly, scale =TRUE)

```

```{r}
fviz_contrib(PCdf, choice = "var", axes = 1)
```

```{r}
fviz_contrib(PCdf, choice = "var", axes = 1:2)
```


####  Variable contributions to PC

The plot below shows the contributions of variables in accounting for the variability to the top 2 principal components, that is, the higher contribution (%) of one economic index in this graph, the greater necessity to including this index into our analysis.

```{r echo=FALSE, warning=FALSE}
fviz_contrib(PCdf, choice = "var", axes = 1:2, top = 9)
```

#### Graph of individuals

Individuals with a similar profile are grouped together.

```{r echo=FALSE, fig.height=9, fig.width=9}
fviz_pca_ind(PCdf, col.ind = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
             )
```

#### Graph of variables

Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.

```{r echo=FALSE}
fviz_pca_var(PCdf,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
)
```

#### Biplot of both individuals and variables

```{r echo=FALSE, fig.height=9, fig.width=9, warning=FALSE}
fviz_pca_biplot(PCdf, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
)


```

### K-means

K-means clustering (MacQueen 1967) is one of the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of k groups (i.e. k clusters), where k represents the number of groups pre-specified by the analyst. It classifies objects in multiple groups (i.e., clusters), such that objects within the same cluster are as similar as possible (i.e., high intra-class similarity), whereas objects from different clusters are as dissimilar as possible (i.e., low inter-class similarity).

```{r}
# Using the results of variable contribution by PCA, I removed two least important indices from the original data.
df2 = as.data.frame(df.yearly)[,c(1:3,5,7:10,12:13)]
# The plot above represents the variance within the clusters. It decreases as k increases, but it can be seen a bend (or “elbow”) at k = 4. 
fviz_nbclust(df2,kmeans,method = "wss")+
  geom_vline(xintercept = 5, linetype = 2)

k2 <- kmeans(df2, centers = 5, nstart = 30)
fviz_cluster(k2, data = df2, palette = "Set2", ggtheme = theme_minimal())
```

## Conclusion

By the means of PCA and k-means clustering, we can classify the historical years from 1967 to 2019 into 5 different economic regimes. Except for 2020, which is an outlier in our case, the first regime is compose of 2008, 2009 (global financial crisis), 2001 (Bursting of dot-com bubble – speculations concerning internet companies crashed), and so on. Therefore, this regime of economic scenarios represent financial crisis. Second Regime is pointed by the Private Housing Units Started Total Monthly Change Index, both 1997-1999 and 2012-2014 are located in this regime. This regime indicates economic recovery. Third regime which is indicated by US Initial jobless claims index and two CPI index, represented by 1974 (1973–1974 stock market crash), 1979-1981(Early 1980s recession).  Fourth regime is dominated by the high value of GDP, Employment Payroll, Industrial Production. This regime is represented by 1976-1978, a period of economic expansion. 
